# Red-teaming LLMs for Penetration Testers

## Introduction

*"LLM red-teaming? That sounds like regular red-teaming. We'll get the pentesters to do it"*

I started this repo to collate information about LLM red-teaming that would be useful or relevant to a penetration tester who found themselves on an LLM red-teaming engagement.
While there may be some similarities in the approach or philosphy between traditional red-teaming and LLM red-teaming (both are adversarial), I think there are some fairly substantial differences also. Penetration testing and traditional red-teaming can be very technical, whereas there's a much higher degree of art / magic / non-determinism involved in LLM red-teaming.


I'm assuming that the audience are proficient penetration testers, have some grounding in technology, and have a basic understanding of at least the fundamentals of generative AI. A glossary is provided in the repo to provide some definition around gen AI and LLM concepts beyond the bare fundamentals.

## Structure of the repo

I've tried to group and structure the information here in such a way to find what you need quickly, and to make the content easy to update. At present the structure is as follows:
- Introduction: This document. Background, structure, caveats and meta-info 
- Threat Modelling: A high level threat model for a hypothetical LLM application
- Test Cases: Based on the outcomes of threat modelling, what you probably want to be testing for
- Glossary: Terms and concepts explained
- Tool Index: Tools that may be useful for LLM red-teaming
- References: Links to related info
- /images/: Diagrams and pictures referenced in the other content
- Misc: Everything else. Notes, scratchings and ideas for future content.
